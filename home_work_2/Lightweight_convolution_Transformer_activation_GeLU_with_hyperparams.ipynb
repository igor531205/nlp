{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/igor531205/nlp/blob/main/home_work_2/Lightweight_convolution_Transformer_activation_GeLU_with_hyperparams.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Выполнил Пушкарев Игорь Игоревич. Группа 23.М08-мм.***"
      ],
      "metadata": {
        "id": "nVmyYXdnD98g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformers."
      ],
      "metadata": {
        "id": "T5wYnQ3Y_pYS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Lightweight convolution Transformer, activation function GeLU, (18 layers, dff = 2048, H = 8)*"
      ],
      "metadata": {
        "id": "MguTu99UQ_Wm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import gc\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 64 # how many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_dff = 2048 # add the hyperparameter dff\n",
        "n_embd = n_dff // 4\n",
        "n_head = 8 # replacing the hyperparameter H 6\n",
        "n_layer = 18 # replacing the hyperparameter layers 6\n",
        "dropout = 0.2\n",
        "\n",
        "torch.manual_seed(1337);\n",
        "\n",
        "# with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "#     text = f.read()\n",
        "\n",
        "from urllib import request\n",
        "\n",
        "# read in all the words\n",
        "link = 'https://raw.githubusercontent.com/igor531205/nlp/main/data/input.txt'\n",
        "with request.urlopen(link) as f:\n",
        "    text = f.read().decode()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input of size (batch, time-step, channels)\n",
        "        # output of size (batch, time-step, head size)\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,hs)\n",
        "        q = self.query(x) # (B,T,hs)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,hs)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_dff):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, n_dff),\n",
        "            nn.GELU(), # replacing the activation function ReLU()\n",
        "            nn.Linear(n_dff, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class LightweightConvolution(nn.Module):\n",
        "    \"\"\"Lightweight Convolution using depthwise convolution\"\"\"\n",
        "    def __init__(self, n_embd, kernel_size=3, padding=1, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.depthwise_conv = nn.Conv1d(n_embd, n_embd, kernel_size=kernel_size,\n",
        "                                        padding=padding, groups=n_embd, bias=False)\n",
        "        self.pointwise_conv = nn.Conv1d(n_embd, n_embd, kernel_size=1, bias=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Permute to (batch_size, n_embd, sequence_length) for Conv1D\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.depthwise_conv(x)\n",
        "        x = self.pointwise_conv(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.dropout(x)\n",
        "        # Permute back to (batch_size, sequence_length, n_embd)\n",
        "        return x.permute(0, 2, 1)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        # self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.lw_conv = LightweightConvolution(n_embd)\n",
        "\n",
        "        self.ffwd = FeedFoward(n_embd, n_dff)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.lw_conv(self.ln1(x))\n",
        "\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "perplexity_log = f'perplexity_train,perplexity_val\\n'\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        if iter == max_iters - 1:\n",
        "            perplexity_log += f\"{torch.exp(losses['train']):.4f},{torch.exp(losses['val']):.4f}\"\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Empty the cache\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "with open(\"perplexity_lightweight.txt\", \"w\") as file:\n",
        "    file.write(perplexity_log)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHbGJkRYZApB",
        "outputId": "22ae12b2-d529-4796-8eb0-a306b30d1394"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "42.805332 M parameters\n",
            "step 0: train loss 4.5496, val loss 4.5505\n",
            "step 500: train loss 0.0106, val loss 0.0111\n",
            "step 1000: train loss 0.0101, val loss 0.0104\n",
            "step 1500: train loss 0.0096, val loss 0.0100\n",
            "step 2000: train loss 0.0095, val loss 0.0097\n",
            "step 2500: train loss 0.0095, val loss 0.0097\n",
            "step 3000: train loss 0.0093, val loss 0.0096\n",
            "step 3500: train loss 0.0092, val loss 0.0095\n",
            "step 4000: train loss 0.0092, val loss 0.0094\n",
            "step 4500: train loss 0.0091, val loss 0.0094\n",
            "step 4999: train loss 0.0091, val loss 0.0093\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# print perplexity\n",
        "df=pd.read_csv(\"perplexity_lightweight.txt\", index_col=False)\n",
        "df"
      ],
      "metadata": {
        "id": "668RVMMItCQ5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "3f437c7c-c62c-44ab-a3da-2d4a9af63672"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   perplexity_train  perplexity_val\n",
              "0            1.0091          1.0094"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ac9461b6-1fcc-4d68-b1bc-ea96a21f87f5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>perplexity_train</th>\n",
              "      <th>perplexity_val</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0091</td>\n",
              "      <td>1.0094</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ac9461b6-1fcc-4d68-b1bc-ea96a21f87f5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ac9461b6-1fcc-4d68-b1bc-ea96a21f87f5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ac9461b6-1fcc-4d68-b1bc-ea96a21f87f5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"perplexity_train\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1.0091,\n        \"max\": 1.0091,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1.0091\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"perplexity_val\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1.0094,\n        \"max\": 1.0094,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1.0094\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "with open(\"more_lightweight.txt\", \"w\") as file:\n",
        "    file.write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))\n",
        "\n",
        "with open('more_lightweight.txt', 'r', encoding='utf-8') as f:\n",
        "    more = f.read()\n",
        "\n",
        "print(more)"
      ],
      "metadata": {
        "id": "VczTiIcdzKYL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e401ebc-0dbb-4abc-d2ee-81489aaad567"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ещ! — —п…\n",
            "ЗдебжЯ, \n",
            "СпБылчеР:\n",
            "ы ачесь!\n",
            "к —\n",
            "Кониц!\n",
            "Р —\n",
            "Всяженцыбкою:\n",
            "\"Бго.\n",
            "\n",
            "СкйеМхю,\n",
            "И — «Лит.\n",
            "Всё ней…\n",
            "Мыла…\".\n",
            "я!\n",
            "Д —\n",
            "Зач)—\n",
            "ЛиПрочья!«РЯ ДляГ…\n",
            "Измою!\n",
            "ЯхмТИ,\n",
            "Здела,\n",
            "Ког:\n",
            "К—гхам».\n",
            "Р О Раздосждым\n",
            "З!Мою Ее!…\n",
            "О…\n",
            "Напшки,\n",
            "\"Д\n",
            "Ли».\n",
            "\n",
            "Стыыл— —\n",
            "И!яжеу.\n",
            "\n",
            "  «.\n",
            "Зак\n",
            "Их».\n",
            "\n",
            "И пичной тпод чтепоь \n",
            "Где ристводить \n",
            "Я кровенно печтель мне рудры пкотал\n",
            "И хошко \n",
            "\"\n",
            "Разми,\n",
            "И гоппроку\n",
            "Поднлешь!\n",
            "Вердце,\n",
            "\n",
            "«С тамкить\n",
            "И чемой. \n",
            "Я ходко нучен, ковенье лодкой Феплидень пишиний ё совешхотное душа од в крод не под сиго тедель то ноело стобрыл \n",
            "Спитною.\n",
            "\n",
            "Хо мены экойцы нем блом, твоелицы жаратьй мыл\n",
            "И чазум под беках знеполпиен. \n",
            "Зета и с туче трень горике,\n",
            "И ли жеы толных еденны зналокой онных, миг. \n",
            "Внаду: в дибица гоблурем хвелиствею на пок от, \n",
            "Отрою, \n",
            "Понефию скразиль\n",
            "Э возй воэтом деленью, \n",
            "Колм — Пашол так! деленно мое!\n",
            "Перкои,\n",
            "И буда льем прижет то пражствой,\n",
            "И в изналатьи\n",
            "Ч редит нушенны Едо К утохне же солюдо\n",
            "И метали жети дали. Пупствальной мы злах глачали,\n",
            "И». \n",
            "И кослуши.\n",
            "Разланны отишин дело б за зеннет., \n",
            "Нида мил морой: казнь\n",
            "Ко праку менчим, душко\n",
            "Где прохреду дело ведер,\n",
            "Задует.\n",
            "\n",
            "С божет брагали сыпотаюти, ль глажди кри свободны стар девал.\n",
            "Двой мереда вреба твовиде вмено тенветь во чела рвехтомной полюто емйа тел етрывной над-вое нах\n",
            "\"Ке теб\". \n",
            "Слевал ним осих.\n",
            "Встве надель теперрий от на овиж я ты ю, \n",
            "И мы мнечни, на члаги!мый тих староц безлого.\n",
            "\n",
            "Ко умилкой превлехлатопитут задро уВцо цы Хедли порето Когой.\n",
            "\n",
            "Не техода\n",
            "С \n",
            "Как огис пот амой ды:\n",
            "Где, \n",
            "Что леда доведи\n",
            "Зочныть\n",
            "И кака, молдинло \n",
            "Не лупредслвых мутавьям\n",
            "Тветил олгда маждит тишу презбе трод ив кот переКан празбоги, ваши торкныхи и , ы, из влазоно.\n",
            "И тохвех мредалож то годец, жолей, \n",
            "\n",
            "Не ночно лочен\n",
            "Пени маченью я мильсть Стих тводною.\n",
            "Не резлобо\n",
            "Твозадвой иденый обритела горьде \"ши вивертваною и лез улостыва и добою,\n",
            "Сладенные разрода, в под мыствою: \n",
            "Где ниут\n",
            "Презвродет, гчасложет\n",
            "И подиными\n",
            "В крама, заль у ми!\n",
            "\"Перно ислит вир:ких не до пахудя споденые злади мфецы, тихростишней.\n",
            "И мы не л перту: нелки инет, \n",
            "Не Парыновичь ув перут\n",
            "\n",
            "В доной .\n",
            "\n",
            "Дадли депер кую твозй деда \n",
            "Где не ме жной иду олечные ватвеввой милоной бездет\n",
            "Их сник мой \n",
            "До переиф\n",
            "И памные Забедикой котот\n",
            "не будет и конещет в бался\n",
            "Где млаке: что лочен ператы очил промае рукое, \n",
            "И зи дыбкой, от — прелостельну. \n",
            "И и де дорном — камолил,.\n",
            "Пекло увуя потбествение! возди, дердвою чербвиней,\n",
            "Душних\n",
            "Но веседо болинел, \n",
            "\"Бам, поллил пыльнитто уч цвелет.\n",
            "Мои звуд прозвек. Гишкие убишку \n",
            "Зат,хи не голиною \n",
            "\n",
            "Их лочко звожины, и в взор, \n",
            "Когде тертелу ков реведны хриза, и менкой зье утури, беды воя груги Сквец\n",
            "И пигил \n",
            "Прику.\n",
            "Глудве боей  мело лвопитал! Ктон уода \n",
            "Мы даши! о вхреда моема похлилы, а дебу о с зрадет,…\n",
            "И вет их\n",
            "Безей! \n",
            "Пелой ремено в взет нетали уполной \n",
            "Клачут\n",
            "Бвых звенова,\n",
            "О злагодоки\n",
            "Пивлено фрик сидене\n",
            "Тельной,\n",
            "И земну рева краденной,\n",
            "И шердценку\n",
            "Ты лябе пим, ваум и глабой леко рообных доне мовти моей пе трей ледар миг, Виканьвойно-свостороми, япрлива, Сершло  стак их левружни в дремой\n",
            "И кде продон росрепечас былео любожи, \n",
            "Сохлыбкись,\n",
            "И тостой.\n",
            "Окримстве эих радною ких  притию ты,\n",
            "И госты,\n",
            "Сого. \n",
            "\n",
            "С ку.\n",
            "Я реждо!\n",
            "Зарагу твуй квуюперелкою.\n",
            "Тих бноже дних и створелем -зона!.\n",
            "\n",
            "Ож тадома,.\n",
            "Президем: не лодине дердцовит. \n",
            "И бреду подов в и терем.\n",
            "\n",
            "Им - темену кушахатыю я руккомхиХ\n",
            "Ижи подой потилю добеник. бледиц \n",
            "Ию не рошум насный годо почет.\n",
            "Зод по, мигех\n",
            "Аих\n",
            "\"Пешелодкистих\n",
            "Когда полилы цевродерны алоть дооны \n",
            "\n",
            "Вашь ух Сыми нье!\n",
            "До не де пыли де-го, недела сабеду. \n",
            "\n",
            "Где — вретли,\n",
            "И -кихимето ласы мразгвечко уа дыкли тоши о муза,\n",
            "Зовать уя, о мною приту верне чути, миреез род, \n",
            "\n",
            "Своких,\n",
            "\n",
            "Врохлаженной Алусде зшине небил окразделе, будю тердцы друки, я до боии вфолонсть уто леррезне, перденых тествой, и одриг татох лиде гугуИ нашиде поллав набе,\n",
            "Как пежны\n",
            "\"\n",
            "\n",
            "Не непред ледный разлоны уж любов и дев моим небим роих своу риде: \n",
            "Заело мы дик мой!\n",
            "Ях крылани дервый подор, сдело чпера тву, \n",
            "Не эм не кех моду зню ней \n",
            "Мел\n",
            "С палогы перем,\n",
            "Себевам их тепои твое зочный кадле год — гор мужщ3ю об ккрит поленные гушу любтихли режи подриго \n",
            "Нековены\n",
            "Имелих оттар стельной подвлавных \n",
            "Но дами гиу предней лведил попродени качны, \n",
            "Ско очене,\n",
            "Нузою в гори федруги, \n",
            "\n",
            "Колво годечно, \n",
            "Не педа не поэт.\n",
            "- что у мле дот веду,\n",
            "Под в едруг бого\n",
            "Есел скоит горидене прорах педо девлю пуженья \n",
            "Пододой,\n",
            "Еще мылени!\n",
            "За мнег душни\n",
            "Сске, либов непах нослай влеса м твек он кхожны\n",
            "И нездоки тему не датровству \n",
            "\n",
            "Стихи разпровы Фед.\n",
            "\n",
            "Гдевно мыду!\n",
            "В главил \n",
            "Жем.\n",
            "В теба в рориценных \n",
            "Скоей! кроп о тели, \n",
            "русто пателно пешед бойний тволо,\n",
            "Из ободы \n",
            "\n",
            "Диши утхрлодой ленник не пят в сестлине полтвой\n",
            "\n",
            "И — у не мноно миньи!.\n",
            "\n",
            "Я моду луша Ве лазих шетной двал, мовым,\n",
            "У лива еиные мнико его — подеды Подледень оных. \n",
            "Я пой прокь кохы в гелу чиконо пыличествиденны, не но латжее погда прела видет.\n",
            "\n",
            "В как евлачолу\n",
            "В пружкон невнок сь: уладели пли во дене истихина, козо задезние хвез тедел доледь!. \n",
            "\n",
            "Предне, не двета времи вердцу голопкою трудтилей грочкосли зниг Ашатото толед, редел полобою \n",
            "Там полго прашу, \n",
            "Иху.\n",
            "\n",
            "Плебны свереело, тылих може!\n",
            "Сед утер мелные двешесть и в тихи, заским доы,\n",
            "Ий клыбкиЭ лушно, В — лих, судкику мне блаж зне. \n",
            "Вдедилы \n",
            "\n",
            "Опомы опин окой ид любвиха!\n",
            "Свеиба год лиженит теший Ровков малны: севли отризримел.\n",
            "\n",
            "И неше бнови девцые ли доло не бодел грез, \n",
            "Кебе не жиз удесь взиечные приким \n",
            "Петаньй рет бозаименнытив векную ребу.\n",
            "Миг кропыше двидишь уки унелитхи перужчу, В тебедо\n",
            "Сладо \n",
            "Зееды\n",
            "Ких ом дод пробиме уждник цы утих,\n",
            "И глемедит я пто посишьны лабрам самомски волнил ль крыла, Кичто\n",
            "И зол пито деашей мы уестних дели о— глачу уседу блав не нерла брого не тамена олго влаввлен горобры,\n",
            "Ди душам, егтих атолн —\n",
            "Ты, юливою стесу и твив стод пене\n",
            "ка, ж летье: овал метенен емлек нвершвои н ок ли твою обиги, коства Столвтрим ливой.\n",
            "\n",
            "То не,\n",
            "И — ни — эвоитым теборвуду зам дена грел\n",
            "Ой,\n",
            "Их груло моей веньъаю мечед угир там\n",
            "Уин дум — слады падела и долвной\n",
            "Крехлщей, \n",
            "Их, \n",
            "Во в перные рвик нашай ребе то фрихи,\n",
            "В мипдоки и седни очхобовать пожкный шеохоться пригруз мы злистих.\n",
            "От где барки мимен левой \n",
            "Пернены\n",
            "Ренры, \n",
            "Яль Пуэтать\n",
            "Паой следотоюх, \n",
            "\"Злоч, \n",
            "Предмае байде кашзо: гибез нечену! \n",
            "\n",
            "Вдуенибу: \n",
            "В зордцы передохлюрин уз моя ут, потда крада,\n",
            "Как не злавым\n",
            "Зведною тих. \n",
            "Ской паэде , праздрежный диле не тарцою бивени рек, \n",
            "С — \n",
            "Где я ожит,\n",
            "И, \n",
            "\n",
            "Втанны одивнымцы царато:, небою — моледны резодит\n",
            "И: отзрас\n",
            "И виден ло пидредье, \n",
            "Скатею душраду даля. \n",
            "Порло хравли не но печешь \n",
            "В клют,\n",
            "И к речаха мно певениу тебе долойою \n",
            "Яго кабуи рапит пылов,\n",
            "И твой, засчаты,\n",
            "И опедой холвен, \n",
            "Беба то мит проты премые Подот окры, \n",
            "И в лазтноки! \n",
            "Квои всердцей\n",
            "И в - Кала кадет Мели впродет  пелиной\n",
            "Подины, пиПлеко это бидуе неущие и дперених дом,\n",
            "В день и Стасы.\n",
            "\n",
            "Евавна немелеру: \n",
            "Твек,\n",
            "Чушке мечкой На вредна свего далопны кроних утерстлы и кружки.\n",
            "Уволой:\n",
            "Сермираю из не преть\n",
            "Или золмью воон!\n",
            "Хром \n",
            "Когка:\n",
            "Пинети,\n",
            "Бодкенны ромох\n",
            "Е зебвою на в в и мне. \n",
            "\"Памоличет но презь, гляде глу, \n",
            "Нвосне биэпится добвойке заток \n",
            "Преду ю рукой, \n",
            "Пела, предько \n",
            "И мез мытарю веле, насуми двезсместо \n",
            "Не ра селий зренны\n",
            "И вудешь милу \n",
            "Дременны дих, додя закожать: глаком он откои \n",
            "Сохласи кем \n",
            "И добрали (упоэтны ум свясно с колной.\n",
            "\n",
            "И гор юж на блека\n",
            "И пам вна клики епо\n",
            "И, друкию горой! де \n",
            "И, двидег. \n",
            "И просает фолпит! Ты \n",
            "Бокражко пригМи, тебе пертавы видкем, о! почей! по девак твет кочды\n",
            "И мне ри-либе молядей бари круже непридвет избой же м удичи душве но монно скву Вендой пистветелые долго\n",
            "\n",
            "Кисшадит \n",
            "И скордиводы хотели жлубидала до рахнодпей ти бреда! безкроствои мидо у ребат не блади,\n",
            "И зни видный.\n",
            "\n",
            "я тобремедел,\n",
            "И грествед в подо где веркысник лишь ножка. \n",
            "Заздо сетатирной, чеэт.\n",
            "Где тихладонхи.\n",
            "Не в схедостих, \n",
            "И, ноте улике это Аето одни мил цвенмой \n",
            "\"И \n",
            "Коривой\n",
            "Спероды ты бисин.\n",
            "\n",
            "Бердций! \n",
            "С ккорамне ни ирь сабодветкой.\n",
            "Зитаньи пладо любая, подной нидал на одохи.\n",
            "Бецствей! \n",
            "Счаснинье моем от и бодою тводевой!\n",
            "Я модкой средож\n",
            "В толых ценья.\n",
            "\n",
            "Схлаву моля Злаги депалезье\n",
            "Везголонод — не призвудельчье, любою — клог!\n",
            "Но паснеди,\n",
            "Не, \n",
            "Зам инех, их\n",
            "Керзеник влега, \n",
            "В в.етил,\n",
            "В вед Ваки правики. \"Не отрверкой \n",
            "И ныною\n",
            "Зах придет оченивЮтвеу первал горой ещено судниц.\n",
            "\n",
            "И пених тех стводох кружты, мы чателкойлех, двер —\n",
            "Запоходожине ихтеины не денною :  нушка \n",
            "Неушчаю детию — тлаго\n",
            "В рествалпа отрем, прошиненин потлиз пленен. \n",
            "И гне празд,\n",
            "Сагодев никры зввоз келенные воде под Солюбиды за вечной. \n",
            "Претма\" \n",
            "Весу \n",
            "Им лезвленой копело й гердпутно равца ием дадор нервы,\n",
            "Не капорокровирков! охниматны Еж деты болно, мысвое. \n",
            "Живрак,\n",
            "И.\n",
            "Бласта, луше Судо таде, любвида, \n",
            "МеСподит зоскы — под тах\n",
            "Жиде отренацивебзхи ниче влествое любвид сениц!\n",
            "И лижене \n",
            "И же мествы гладье\n",
            "И пловей прак мяхотичей морами пенны\n",
            "Тусы!\n",
            "И ды Фал двею \n",
            "Ил на одо мласел — могаховите че льза дродит.\n",
            "\n",
            "В это веклок топрого когда шада не вдроза\n",
            "Их,\n",
            "С мечке дворат Прешье от нем их безниход подом лже сверл дон грузгил\n",
            "И, \n",
            "Во преме вепег шемныйа утворошли —!\n",
            "\n",
            "Пероги мом от разлвато мениту, \n",
            "К голодиц келне пететили годим\n",
            "Дошри ж ночели нее\n",
            "И б Пын удерехли другь: \n",
            "Пеешеги оной зеелих мурен: з евиделы то инудружны любе славда полот Еменен!\n",
            "Что росу окиберла и неравной,\n",
            "Вокои горнепуе мой ук и ла ленны: - блетати я радостово мето утвое уййска сиэмо\n",
            "И хвук дотою\n",
            "Им,\n",
            "Порит годиш глуч предеси когли пвешно: \n",
            "\"Прылац во девет двез, ь и степрудвоей,\n",
            "В был окии бежу влежное всё \n",
            "Кочетливишин — - теброские подой воьде (матумны, коллин в педой их елы,\n",
            "Я был ретем ниветли пит отды ле рохльди твой он и влег,\n",
            "И глца,\n",
            "Нузикно, пресень окороченично не шаен тебе мой не рнем уду наший и ких длаж.\n",
            "\n",
            "Во точ ю зана\n",
            "И. \n",
            "Не — стовонно овод свой, калоне миде всех. \n",
            "Кака гор ом глее буд продвос лельть и бедливой\n",
            "Евидногам изнота,\n",
            "Мы лубою колго. \n",
            "Поотрапи взишь ведет.\n",
            "\n",
            "Япеюрсь негом быле паствою то граду утгро хожких тво\n",
            "Приспыло в ноч\n",
            "Вовидела\n",
            "Их точь,\n",
            "Им срами от почпотном пощко \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "В мел дебу накрозем,\n",
            "И я мое дроввостали длись:\n",
            "Кой тем,\n",
            "Тих дочю.\n",
            "\n",
            "И мой зетих ей бодит.  втих, \n",
            "В либрозмана пеэт слих не сочей виценно шрос пенал и ес это борновой стевеца презо моло молто глазы душке закел \n",
            "С дешую печно,\n",
            "Вак е еденье их дур, \n",
            "И поэтих хате\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R_vzbCQBzKRa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}