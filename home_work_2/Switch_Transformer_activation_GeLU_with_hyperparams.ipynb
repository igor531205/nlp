{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/igor531205/nlp/blob/main/home_work_2/Switch_Transformer_activation_GeLU_with_hyperparams.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Выполнил Пушкарев Игорь Игоревич. Группа 23.М08-мм.***"
      ],
      "metadata": {
        "id": "nVmyYXdnD98g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformers."
      ],
      "metadata": {
        "id": "T5wYnQ3Y_pYS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Model Switch Transformer, activation function GeLU, (18 layers, dff = 2048, H = 8)*"
      ],
      "metadata": {
        "id": "biPxRF5VQmFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 64 # how many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_dff = 2048 # add the hyperparameter dff\n",
        "n_head = 8 # replacing the hyperparameter H 6\n",
        "n_layer = 18 # replacing the hyperparameter layers 6\n",
        "dropout = 0.2\n",
        "\n",
        "torch.manual_seed(1337);\n",
        "\n",
        "# with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "#     text = f.read()\n",
        "\n",
        "from urllib import request\n",
        "\n",
        "# read in all the words\n",
        "link = 'https://raw.githubusercontent.com/igor531205/nlp/main/data/input.txt'\n",
        "with request.urlopen(link) as f:\n",
        "    text = f.read().decode()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input of size (batch, time-step, channels)\n",
        "        # output of size (batch, time-step, head size)\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,hs)\n",
        "        q = self.query(x) # (B,T,hs)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,hs)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, sequence_length, embedding_dimension)\n",
        "        batch_size, sequence_length, _ = x.shape\n",
        "\n",
        "        # Compute Switch Gates\n",
        "        switch_gates = torch.zeros((batch_size, sequence_length, self.num_heads), device=x.device)\n",
        "        for i in range(self.num_heads):\n",
        "            switch_gates[:, :, i] = torch.randn((batch_size, sequence_length), device=x.device)\n",
        "\n",
        "        # Normalize Switch Gates\n",
        "        switch_gates = F.softmax(switch_gates, dim=-1)\n",
        "\n",
        "        # Compute attention outputs for each head\n",
        "        attention_outputs = [head(x) for head in self.heads]\n",
        "\n",
        "        # Apply Switch Gates\n",
        "        switched_outputs = []\n",
        "        for i in range(self.num_heads):\n",
        "            switched_outputs.append(attention_outputs[i] * switch_gates[:, :, i].unsqueeze(-1))\n",
        "\n",
        "        # Concatenate and project\n",
        "        out = torch.cat(switched_outputs, dim=-1)\n",
        "        out = self.proj(out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_dff):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, n_dff),\n",
        "            nn.GELU(), # replacing the activation function ReLU()\n",
        "            nn.Linear(n_dff, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd, n_dff)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "perplexity_log = f'perplexity_train,perplexity_val\\n'\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        if iter == max_iters - 1:\n",
        "            perplexity_log += f\"{torch.exp(losses['train']):.4f},{torch.exp(losses['val']):.4f}\"\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "with open(\"perplexity_switch.txt\", \"w\") as file:\n",
        "    file.write(perplexity_log)"
      ],
      "metadata": {
        "id": "ePKeKE6RrTDq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f483afb4-ff4b-4593-b591-b383bd6c2df0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "39.170388 M parameters\n",
            "step 0: train loss 4.4587, val loss 4.4553\n",
            "step 500: train loss 2.2999, val loss 2.3512\n",
            "step 1000: train loss 1.9064, val loss 2.0288\n",
            "step 1500: train loss 1.6691, val loss 1.8263\n",
            "step 2000: train loss 1.4822, val loss 1.6841\n",
            "step 2500: train loss 1.2897, val loss 1.5377\n",
            "step 3000: train loss 1.0524, val loss 1.3602\n",
            "step 3500: train loss 0.7480, val loss 1.1211\n",
            "step 4000: train loss 0.4414, val loss 0.8334\n",
            "step 4500: train loss 0.2393, val loss 0.5322\n",
            "step 4999: train loss 0.1511, val loss 0.3095\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# print perplexity\n",
        "df=pd.read_csv(\"perplexity_switch.txt\", index_col=False)\n",
        "df"
      ],
      "metadata": {
        "id": "WEsnVmmvz1Nu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "ec84b4c4-bee9-49ed-bbab-f57bea932cce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   perplexity_train  perplexity_val\n",
              "0            1.1632          1.3627"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d6f9f10d-b6ac-4a86-9364-f950afac3ae9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>perplexity_train</th>\n",
              "      <th>perplexity_val</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.1632</td>\n",
              "      <td>1.3627</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d6f9f10d-b6ac-4a86-9364-f950afac3ae9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d6f9f10d-b6ac-4a86-9364-f950afac3ae9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d6f9f10d-b6ac-4a86-9364-f950afac3ae9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"perplexity_train\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1.1632,\n        \"max\": 1.1632,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1.1632\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"perplexity_val\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1.3627,\n        \"max\": 1.3627,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1.3627\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "with open(\"more_switch.txt\", \"w\") as file:\n",
        "    file.write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))\n",
        "\n",
        "with open('more_switch.txt', 'r', encoding='utf-8') as f:\n",
        "    more = f.read()\n",
        "\n",
        "print(more)"
      ],
      "metadata": {
        "id": "7e-ftDU2z1J5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1517b2b7-c6c4-4371-8be7-e5c19dd089ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Увы! зачему всё таы своим слуг,\n",
            "Что с ней сильнем ни пор дарил\n",
            "Мой предовствовал иным, сколь меня\n",
            "Когда ступайте венок.\n",
            "Смешон, право, нам и доброн,\n",
            "Дурено без чемом, не заметно.\n",
            "Счастный лампадицы,\n",
            "Парнасских лет ак ферикуновал он,\n",
            "В черных Аферс с Гписалам\n",
            "В ними дух селый Вакха не попал\n",
            "Не предстал по хвалу за Парнасса.\n",
            "Ты ль свои в тьме го проведете Ясном дан России\n",
            "Их на чуждыха благородных вод.\n",
            "\n",
            "Море зварвенья бранный прах,\n",
            "Предвижу грозного врага,\n",
            "Воевы грознога лежит лес благорова:\n",
            "О, а сладко жреби и гриапа\n",
            "Гирает лестины сверней кровавы\n",
            "Звезда и лет лож десницея зовитель\n",
            "Стоятся гранчанный в золою игроев,\n",
            "Кривою на гразные землега, в печальные лежной,\n",
            "Кругом склонитель быстрый клеветы бег широкой,\n",
            "И с него лудовать вспятую даль,\n",
            "А что вечор оди за нею той светлует\n",
            "Чуть слезы лее услышится емож боле — как те, волны товариду.\n",
            "Услышал бури ити моление древни,\n",
            "Что значат радости моей женило\n",
            "Один пришель судьбой сердечнествие любил.\n",
            "В молчаньи бароне и лежит во один не очей,\n",
            "глянул св »д сокрылись мрачны очи\n",
            "И горе надо шагольной вынулись,\n",
            "И пошел бил их ладных бурь нас\n",
            "гореститель их высота,\n",
            "И наши рифмы наших стадаленний,\n",
            "Среди вас восточных весельй\n",
            "Потомствовал на грозну стаем.\n",
            "Детичественный мудрый: про милый,\n",
            "Он вдруг наш смерти весне.\n",
            "Когда тумрут в пред ночи солнце созданье,\n",
            "В ее девсти своей, и в изображенье,\n",
            "И закован сердце вновье твоей.\n",
            "\n",
            "Ты был праздное, горишь мне\n",
            "Во мне пенаток себе нам ночи заблужденьем\n",
            "Священной забвенью песни мира,\n",
            "Отрадаленных друг себя веселья,\n",
            "И, милых певцу бессте забывал,\n",
            "В их ласное селых и я ни могилые,\n",
            "И сердце мирных даров излил блистают,\n",
            "Дрожали всякой светлый вздох лет виных заедал.\n",
            "\n",
            "Все храм вострахнул он могучий Олег.\n",
            "Он с нами с подружиной весь бежал,\n",
            "Чуть выше бродят, и мир землезные сени,\n",
            "Всё смуты глаз ожестовой клики.\n",
            "И назов шел Парнасса.\n",
            "Позволивала вося! Но, спасибо, братец суходства,\n",
            "Желанья богов благополучный\n",
            "Я готов кубок побед, с тобою,\n",
            "И будешь тебя во мире я равен был теку.\n",
            "\n",
            "Я славный певец, пирон молод, и живойный свет\n",
            "Сквозь юные, и живый и мрак\n",
            "Не сердцем назнать уж не обижален\n",
            "Обманывать милей.\n",
            "\n",
            "Кто, утра я слышу и тоску!\n",
            "Когда, на цвете догнавидывал он,\n",
            "Говорит его свирепел,\n",
            "Покамест с того сторон родногою водою,\n",
            "Из утренний сгряды в серебровом шостоин.\n",
            "Старик молитесь на веры свете:\n",
            "Лишь уткрыла тарь, охранится\n",
            "Голова Ерусали в не попал.\n",
            "Черный брань на севелье убийской,\n",
            "Без подобрен и блещет.\n",
            "Невольный гимн. Перед которок и белы,\n",
            "Ты здесь, визобра за днями\n",
            "Лишь поэтической цветки,\n",
            "Увидел в чаше косы \n",
            "Пускай Наташам виногради. \n",
            "Царь не передалки меня. но ль не льзя плакал. \n",
            "\n",
            "шетвой проказал менного катила, \n",
            "Проходит он, но  их вижу, признаюсь славой юбою, \n",
            "И не заграды над Сева я большего села, \n",
            "Спорить его как-то не послушен, \n",
            "Благородица на брегах брадатым уберным \n",
            "Как могу на отца: в зеркал монарный дорого, \n",
            "Да черн кападшей под обевом \n",
            "Взад больша коня с юда заснул. Кто же \n",
            "Отдай головую славку любовь. \n",
            "МЧто приятно перед ясный друг! \n",
            "Томитель мой Аристоф нец \n",
            "Ты разной священ милый, \n",
            "Чем, надеясь лих наботник ты, \n",
            "Страши весельее, безумный, \n",
            "Стал обзание! лени и славой, \n",
            "Играй покрытый круг, \n",
            "Кругом прелесть неправой самой. \n",
            "Но любви — сетущая свобода, \n",
            "Покровавала тих вечерняя звезда, \n",
            "В те Веселье прилась балоу Годфира, \n",
            "И Парижила Марка и Весть годна года. \n",
            "Сам их без славой не был: \n",
            "Хотелось моих послал приветал. \n",
            "Безмолвно повитывалось, \n",
            "Счастливца мой венец, \n",
            "Свершилось, для тебе советилось \n",
            "Спасить естропа \n",
            "Меня не пользы нет души замира \n",
            "И веселость Аполлона \n",
            "На колыбели Тасса \n",
            "Благов мирной утеса. \n",
            "Думати беззаетное поле, \n",
            "Даменною повитель, \n",
            "Не гожичься туже ни сонрла \n",
            "Сквозь мирною ревнивой, \n",
            "И балагодать горде. \n",
            "Какой искал же напитою \n",
            "Оба знатном и шалой \n",
            "И воспоминататся. \n",
            "\n",
            "Чья ропокин усердной \n",
            "Стоит беспечной дремлет: \n",
            "Найдешь темный Главки набот! \n",
            "О встреча утренний Парижу \n",
            "И скупит, под зашам. \n",
            "Он был и он меня злых головою \n",
            "И сердцем и долгой милот заброше. \n",
            "\"Чем шаль, послушать не видно» — он блио. \n",
            "Сей человек Итак, красавиц мол, \n",
            "Трое славы поток заметила \n",
            "Из холодного косне. \n",
            "Но к брегам почестилась \n",
            "Иные варильно в тьме ночи \n",
            "С дыханулась мы цепь, \n",
            "И ты под крыльем и шумли! \n",
            "\n",
            "Ты поступь волночь его не \n",
            "И взорожила до дночая \n",
            "Вздохнула заря. Она былая \n",
            "В день мужика миг наих \n",
            "И потшел славных Муз и куда \n",
            "Умей тогнали уж наследила. \n",
            "И скрылась ее слилась мерти ночи \n",
            "Где межданья памяти на месте, \n",
            "Вы сокой храмор лестеры, \n",
            "Друг не замедли меж ны, \n",
            "Вся провожу я зыкорных \n",
            "С кострякой шапкою казною \n",
            "И себе и пени. \n",
            "\n",
            "Зачем гости наследний \n",
            "Блещет круг да красных очей будь: \n",
            "Не вот уже без наклонный, \n",
            "Обманения пожален, \n",
            "Его громко бьет видать \n",
            "И колдунью превраток \n",
            "Он всебе не посниму \n",
            "И тебе нрашить новый красота \n",
            "И так искус муще молодых \n",
            "Лучеко войны повторяешь. \n",
            "\n",
            "И он мне грустно, несчастной другой \n",
            "Набойку образ говорит. \n",
            "Обойно стигнуть мой \n",
            "Невозвышенных жизни нет — \n",
            "Увы! Но предаемся, милый мой сон, \n",
            "Подозром мне дан русского каймелька, \n",
            "Он зай мой мил, мудрость и спасенный соблюдатель. \n",
            "Ты думал: во мгле час \n",
            "Требовые творенья, \n",
            "Опозабудь его стесный держат, \n",
            "То к брегам погодут обороза.\n",
            "Прозерпина в пылу разрывает \n",
            "В уне грудь его взор. \n",
            "Людьма обнажил \n",
            "Без нас отчаянной одевы \n",
            "Впервы послания сильная \n",
            "Семейнив н се летала и расприрос \n",
            "Друга пред ними нучей \n",
            "Не плывет угрой. Убийской день милости женой \n",
            "\n",
            "Были элегийской благ и мне души родные, \n",
            "Или вы и  с нашей кругом благоры, \n",
            "Дымились с началась мы, \n",
            "Сердца и скипели России мирто в слух прелестны \n",
            "Летели пальмы пред лип грозою выповы. \n",
            "Здесь полные тей нет лежал. \n",
            "Пробудил свой и снова друг \n",
            "Поникнула томны очарованный, \n",
            "И ваши творенья парус рок он, \n",
            "И брит незаменил я без гордился \n",
            "Сень я кровью признаюсь, милых думал. \n",
            "И сей новую торжественную разлученну,, \n",
            "Отныне слугу, братствую твердость примир мирной златую.\n",
            "Мальвина, над Летовки жас запречь и звон Ужасом, \n",
            "Священний тутомительная в пучина \n",
            "Над шпоры волны очам, \n",
            "На кольне видно - стали испали. \n",
            "С тогнувшись мы, то смыслился, \n",
            "Но мы  тот час самое соненний \n",
            "Ночью нескромной Скажите: \n",
            "В наши здесь на поле, \n",
            "Она полно доброго царяет, \n",
            "Над озеро утра убава, \n",
            "Как сладостно хозяин \n",
            "В своих червенных санах \n",
            "Онах Дни могущих пожина. \n",
            "Они боидный друг нежных мир в костей путь: \n",
            "Без поносаторгает и богу, \n",
            "Он заснуть спасет. Он покойно не чуенье лишит. \n",
            "\n",
            "Но скажут, каждый воздыхает, \n",
            "Меж дерзоил друзей мирную судьбе. \n",
            "Свой часов ли тихой гений спит, \n",
            "Для радони палоты счастым жемчуг \n",
            "Не скоро ли она встречи \n",
            "Рече гоненной Энибель \n",
            "Для достоин Идеовских я много \n",
            "Нераз, надоеловал свобода, \n",
            "Она сиделися глупца \n",
            "Ему не донносила Свободы \n",
            "Глаза полной пещеры хлопал тлес \n",
            "Везде ты, легкой рабстала \n",
            "И крепко свои стрелы свирепуть. \n",
            "Ты любишь зовешь, милый друг, \n",
            "Презренный, мрачный и уголок, \n",
            "Воспетнее меч и славой был душой, \n",
            "Покоем, сжатым усыпленным зимениной \n",
            "Волшебной груди не тайной \n",
            "Долго, небом Гражденный, волшебный, \n",
            "Во тьме для сенатого свобода, \n",
            "Под беспечною стройных очей \n",
            "Седой издали мен был. \n",
            "Кто слава богу, таков певец \n",
            "Тебе, во влече злог я б ибранный, благодать без \n",
            "Над озеримат — умом терзаемых. \n",
            "Ты помнишь ли, сказал, там покрый пыл свершит \n",
            "Назакойно вто коснулся он. \n",
            "У нас Подноси, милой кумирный, \n",
            "В мольбой отразил Монах мятежных \n",
            "Херувиды от скукой бронаНы почивал картины: \n",
            "Хоть я любил твоей признанье \n",
            "Моей жизни разон было ль любви, \n",
            "Но только от малодушно пированье, \n",
            "Мне нравится ты мне. \n",
            "\n",
            "Священник\n",
            "Предпочему в этим кровавой доленопражай \n",
            "Свою восторгу оживаешь \n",
            "И отвагу юных днера \n",
            "Душа милость, милость мою младое. \n",
            "Но было жив безумных отца \n",
            "Не могла мирный дух пороческик \n",
            "Коварстно в груди несчастной наук \n",
            "(Не льзя рифмы прежних славного любил \n",
            "Себе лет — а б раньство награда \n",
            "В ночь - выходят и на брега тыка \n",
            "О сщарил яружья вдруг бродящие сосирских \n",
            "И их сель ружа. На ложе суховой \n",
            "Он в поле жизненный, над самахами. \n",
            "\n",
            "свои, утопленные брегами, \n",
            "Обожались наукрыл ты \n",
            "Но с енью дальних недругомы \n",
            "И на час олнце е летит \n",
            "И в сумрачной зов ярога \n",
            "Взялся души звездою. \n",
            "Кинув друг чужой, \n",
            "Как некий посетил \n",
            "\n",
            "- сладостный! Честолитвый, \n",
            "Взошли в пустучись молодой. \n",
            "\n",
            "Победились чудный год \n",
            "Уже слышит и смехо злота \n",
            "И от народ ело слегким \n",
            "И весь ему поравить: \n",
            "— \"На струине брегу не \n",
            "Из госпоромим, \n",
            "Все от шибка знатном \n",
            "Несчастный долго поклонился, \n",
            "Своих од кедивляется, \n",
            "Не посыпит слез, мой волне. \n",
            "\n",
            "Но чело был ее слезы и как томный пал! \n",
            "Воды слезы признайся, \n",
            "И Пушер велики нечести так \n",
            "Доходи мень вам стесняют. \n",
            "Какое ты живо мне право море \n",
            "В Я прелесть вами я наслажденья, \n",
            "Мы милая мне изни молвые \n",
            "Одна надежда! но и томили \n",
            "В безумер полях очей, \n",
            "Так искорей и зефиры \n",
            "Для с нами томных волненией тоской, \n",
            "Для слиами надеждою прекрасна, \n",
            "Понимая шутя во мечтах венец! \n",
            "Одна, так звери, мой будет с тою!. \n",
            "Ах, он прелесть найдет, \n",
            "В посте, под кем российский раз \n",
            "Сложил и ты, отцовалНенистый без латынут. \n",
            "Тоскар не чувствуешь ли, \n",
            "Без правду показ негу, \n",
            "Рассказ тебя, не бессмысленник, \n",
            "Мне жаль обещали своих, \n",
            "Если правдивала стиха, \n",
            "Веселых излокованцев стихов, \n",
            "Благотворных Грешин украдком, \n",
            "Поп венчанный Царско-сельской спред ним \n",
            "Зать в уныние гребц \n",
            "Вперилась она, как бессмерть величавала \n",
            "Реки сердце полдневнопослали. \n",
            "Но взор один ведаль посвящен Героя\n",
            "Ему красавица не видит \n",
            "Свой долг красоты кукорыла \n",
            "И песни оживляла пустын, \n",
            "Как грешник обнажилась она \n",
            "И жаркое весленного трепещено. \n",
            "\n",
            "С тобой глубой поразденный, \n",
            "Всё вместитель стон, \n",
            "Всё тот жизни надо мной. \n",
            "Забыло мне открат отперятно. \n",
            "Как легкой певец \n",
            "Тебе вышит полосата \n",
            "И сладострастный сургуч девыкой, \n",
            "В молчаньи смятеньи слезах \n",
            "И тяжет песни ранний лени \n",
            "Непонимого возмущает \n",
            "Во цвете юности скуки \n",
            "Поникнул в веры ст поберный челной \n",
            "Он. беспечно сюда влечет \n",
            "Он разбирал в очи с небесной \n",
            "Он вдался в его со дремлет\n",
            "И схолмнов заключен был в и не похваливался. \n",
            "Подъемлющих осторожный \n",
            "И не милых жертел почтих день прямой. \n",
            "Он любил я ж: Где я вильной светлых наслажденья,\n",
            "Я знал аскользы прекрасный и печали, \n",
            "И сладко хвалением и любви!… \n",
            "Восклицепи легкой благослонный!\n",
            "К мечам! волны лета чредою красотыми, \n",
            "Сто\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J3_p-Uqvllye"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}