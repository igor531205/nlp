{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/igor531205/nlp/blob/main/home_work_2/Evolved_Transformer_activation_GeLU_with_hyperparams.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Выполнил Пушкарев Игорь Игоревич. Группа 23.М08-мм.***"
      ],
      "metadata": {
        "id": "nVmyYXdnD98g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformers."
      ],
      "metadata": {
        "id": "T5wYnQ3Y_pYS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Evolved Transformer, activation function GeLU, (18 layers, dff = 2048, H = 8)*"
      ],
      "metadata": {
        "id": "F7djBATgPlZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import gc\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 32 # how many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_dff = 2048 # add the hyperparameter dff\n",
        "n_embd = n_dff // 4\n",
        "n_head = 8 # replacing the hyperparameter H 6\n",
        "n_layer = 18 # replacing the hyperparameter layers 6\n",
        "dropout = 0.2\n",
        "\n",
        "torch.manual_seed(1337);\n",
        "\n",
        "# with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "#     text = f.read()\n",
        "\n",
        "from urllib import request\n",
        "\n",
        "# read in all the words\n",
        "link = 'https://raw.githubusercontent.com/igor531205/nlp/main/data/input.txt'\n",
        "with request.urlopen(link) as f:\n",
        "    text = f.read().decode()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input of size (batch, time-step, channels)\n",
        "        # output of size (batch, time-step, head size)\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,hs)\n",
        "        q = self.query(x) # (B,T,hs)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,hs)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_dff):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, n_dff),\n",
        "            nn.GELU(), # replacing the activation function ReLU()\n",
        "            nn.Linear(n_dff, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd, n_dff)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class ImprovedLayerNorm(nn.Module):\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.gain = nn.Parameter(torch.ones(features))\n",
        "        self.bias = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.gain * (x - mean) / (std + self.eps) + self.bias\n",
        "\n",
        "\n",
        "class EvolvedBlock(nn.Module):\n",
        "    \"\"\"Улучшенный Transformer блок с дополнительными skip-connections\"\"\"\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadAttention(n_head, n_embd // n_head)\n",
        "        self.ln1 = ImprovedLayerNorm(n_embd)\n",
        "        self.ln2 = ImprovedLayerNorm(n_embd)\n",
        "        self.ffwd = FeedFoward(n_embd, n_dff)\n",
        "        # Дополнительные skip-connections\n",
        "        self.skip = nn.Sequential(nn.Linear(n_embd, n_embd), nn.GELU())\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.skip(x)  # Дополнительное улучшение\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        # self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.blocks = nn.Sequential(*[EvolvedBlock(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "perplexity_log = f'perplexity_train,perplexity_val\\n'\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        if iter == max_iters - 1:\n",
        "            perplexity_log += f\"{torch.exp(losses['train']):.4f},{torch.exp(losses['val']):.4f}\"\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Empty the cache\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "with open(\"perplexity_evolved.txt\", \"w\") as file:\n",
        "    file.write(perplexity_log)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsUk7hzX1ENu",
        "outputId": "c6267129-6b3e-4faa-d844-a6fec0546863"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "61.661268 M parameters\n",
            "step 0: train loss 4.4700, val loss 4.4664\n",
            "step 500: train loss 2.6257, val loss 2.6484\n",
            "step 1000: train loss 2.3579, val loss 2.4030\n",
            "step 1500: train loss 1.8860, val loss 2.0153\n",
            "step 2000: train loss 1.6151, val loss 1.8041\n",
            "step 2500: train loss 1.4137, val loss 1.6480\n",
            "step 3000: train loss 1.2155, val loss 1.4993\n",
            "step 3500: train loss 0.9979, val loss 1.3368\n",
            "step 4000: train loss 0.7616, val loss 1.1279\n",
            "step 4500: train loss 0.5262, val loss 0.9122\n",
            "step 4999: train loss 0.3396, val loss 0.6672\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# print perplexity\n",
        "df=pd.read_csv(\"perplexity_evolved.txt\", index_col=False)\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "2O_yHvddlDMq",
        "outputId": "28575598-036c-48df-fdc9-8671d80779a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   perplexity_train  perplexity_val\n",
              "0            1.4045          1.9488"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-52e2d275-515a-4473-a738-bb6c9140e1af\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>perplexity_train</th>\n",
              "      <th>perplexity_val</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.4045</td>\n",
              "      <td>1.9488</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-52e2d275-515a-4473-a738-bb6c9140e1af')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-52e2d275-515a-4473-a738-bb6c9140e1af button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-52e2d275-515a-4473-a738-bb6c9140e1af');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"perplexity_train\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1.4045,\n        \"max\": 1.4045,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1.4045\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"perplexity_val\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1.9488,\n        \"max\": 1.9488,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1.9488\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "with open(\"more_evolved.txt\", \"w\") as file:\n",
        "    file.write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))\n",
        "\n",
        "with open('more_evolved.txt', 'r', encoding='utf-8') as f:\n",
        "    more = f.read()\n",
        "\n",
        "print(more)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSRrYaMUlC9w",
        "outputId": "0a3ee500-e6ca-427c-fd4b-edf8c812329e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Один моей подишь верный свет\n",
            "В тени пору дружествует любезно тебе:\n",
            "   И сладострастья во мгле уже тесекла\n",
            "Воображенью для мелькаю странец младость\n",
            "   И не заменило во мне\n",
            "   Он вдохновенный Гангет,\n",
            "И у видел, как во стесне ночной грек,\n",
            "   И тайно вперяду на вас,\n",
            "   Минуту даром, и стене рядком\n",
            "   Дрожит плана и москолоне.\n",
            "И право, их благословит.\n",
            "Меж тем, как вас отдальный путь\n",
            "Так исцело пред ним составит.\n",
            "Он мрачнее слезы мне страшно,\n",
            "И вас мертвец сокрылось,\n",
            "И тень, мертвецки поседелый,\n",
            "Где дсворил вас издавит повеса.\n",
            "И круглый Свобода из-звонки,\n",
            "Пойдем бесмертный домик,\n",
            "И я табок полночный ваш!\n",
            "Приди, Восторг в нем красенце,\n",
            "Вдруг монастыря сонны.\n",
            "\n",
            "Твой ключ вонюшний звон милый,\n",
            "Моглы гений его смиренный.\n",
            "Смирись! Трижды красноречив —\n",
            "Вселенна в нем употренную,\n",
            "С ревой потравию несчастную руку.\n",
            "Ты знаешь: она она ты прекрасна,\n",
            "И в ненас вдруг увидел своими грустыми,\n",
            "И изрелася венчая в дальнеми…\n",
            "\n",
            "И вдруг усла, и в рот санки верь молвил\n",
            "Она раскается, и нежно славу\n",
            "Знакомый ревнивую лица…\n",
            "Ужель обняла кровь струи ты любишь в есиил\n",
            "И нет, о стих взяла на тебе меру.\n",
            "Ты нас северную день восхищал\n",
            "Уж видит сердце на искалось над Тигрий\n",
            "Пред Локаню склонилася семятся в пустыне,\n",
            "И лынче свидел хладный и год совет\n",
            "В своем темном уголке безвестно сливу,\n",
            "Ответела нас Алогонского блужденья!\n",
            "Спешите пришет гибельный час:\n",
            "Нет! где может рассветившись ветвер,\n",
            "Востреляся придворный взор его пламя,\n",
            "В модной дремлет нежностью стени ангенстин.\n",
            "\n",
            "Правдой младости мне страшной соною,\n",
            "Невозможно рано волненье спят окроет\n",
            "Протирать отчаянье беднятся,\n",
            "Простите, пред Живопии мечтать.\n",
            "Вы сладострастные вечерней тишине,\n",
            "Медвиже красавицы повелом,\n",
            "Разрублял твой тебя, я вещудый смире\n",
            "Овира тихий деков завистливу грудь,\n",
            "Чистый верен, облические соненравный,\n",
            "Зевезет над дистих некий мой поцелит пустыннет.\n",
            "\n",
            "И говори: Он рабственья судьбе взор отпиненья,\n",
            "Взяв гроза его — я промолвил надежды\n",
            "Для бури севера, возьил страстей.\n",
            "\n",
            "И в нем растрашеньях бедняках\n",
            "От неизменные княхтел,\n",
            "И видел потух и забвенья\n",
            "Сердце душ вистое мне\n",
            "Тоске предалагаш(служил.\n",
            "\n",
            "Я вот, как жизнь, и ты, воспил\n",
            "И неба, забывшенные напевы,\n",
            "На им попр горамони своем\n",
            "И в морской соседей твоей\n",
            "Виленьи постни святы.\n",
            "\n",
            "Позволь стал студента прямой.\n",
            "На воеводстве, на страха, все поле\n",
            "Порастется дозором!.\n",
            "А вдруг умен, увы, сказать,\n",
            "Но ну всех гром, ты пошутка,\n",
            "На тропинуть у далину,\n",
            "Ты берега, холм забот\n",
            "И ташу всегда покинуть…\n",
            "\n",
            "На дороге, бедный не дух…\n",
            "Ты редко, редко любовь!\n",
            "Убегая ревнивую длану,\n",
            "Упьюсь к ней чести красу,\n",
            "Но сам обереченка чужого\n",
            "Мы всем над ним и скачу:\n",
            "Нас ним Иль подпрога желанье.\n",
            "Вся тогда себе к друга,\n",
            "Себя отдворил, я пьюный ядря,\n",
            "Часте златки грозны и гиний.\n",
            "Здесь точно, суд бога ради,\n",
            "Погаснет меня и не забываю\n",
            "Веселый пустын мир воображенья,\n",
            "Скитаться втероженный толп,\n",
            "Главу в след Элизий тихо Россия.\n",
            "Теперь не мне: Увижу: лицо лёхия и тобоже!\n",
            "Она все любить. Оно слез день Закона\n",
            "Найдите лить, никогда от морей\n",
            "Тебе жизнь, а бога рука друга,\n",
            "Так лелеят она с жалега.\n",
            "В краю чуждый мне долг сорвета,\n",
            "Пленявши девственница запроведа.\n",
            "\n",
            "Могучею в царствие на чертогая несет,\n",
            "Погини блеск, и милые друзья,\n",
            "И пьет в дни, дети, вьются князья, вьет\n",
            "И толький дальношой свет\n",
            "Воссторг по мертвецом живеет\n",
            "Где Карала под Ижаровать, \n",
            "Бледнел идет Бову удались.\n",
            "\n",
            "Вдали ты масул заклинанил, \n",
            "На голове удалок \n",
            "И холоп, теской ножек,\n",
            "И волнами несчастной\n",
            "Нестолюбивой плача…\n",
            "\n",
            "Приближьтесь….\n",
            "Вот время когда ты знаешь,\n",
            "Откровенная мне внимал\n",
            "Тебя и холоп, отворена.\n",
            "Ел Любит желанины твои.\n",
            "Поди, легкой домик мой милый,\n",
            "Быть мочит, ходя твой сон\n",
            "И точит сон, Радико\n",
            "А там прекрасную омрачило…\n",
            "И тише промчался в хлынет,\n",
            "И мещанье светлые могилы\n",
            "Но сын, темною порой\n",
            "И как бурей записной\n",
            "\n",
            "Моих не проздник толчи\n",
            "И здравый ум таской свет\n",
            "Спесни под на поясен.\n",
            "Нет, ларек, где Москву,\n",
            "(Ницейской дружный мрак час\n",
            "Быть спешит в часы поле\n",
            "Мурлым жгордец песнь дорога\n",
            "У веселый царей.\n",
            "\n",
            "Пора воспомнист Княжнин,\n",
            "Бесстыдный брег любви:\n",
            "Веселый божественный!\n",
            "Гора отца ей, поэт.\n",
            "Оба воспевала тревожит!\n",
            "И всех чувствую навек,\n",
            "Вирите безвестную силу,\n",
            "Тростник благо, Державина бога,\n",
            "Глава гроб и звение мрачное тень,\n",
            "Потихнуть ароел, я в необщем рожденье\n",
            "Узнав, писать ты сам садеть себя\n",
            "Но служай нам себя: не гибнешь ты\n",
            "Для нам незабвенных невольно слез власть.\n",
            "\n",
            "Не для томного, слепой не через \n",
            "На опален растави света.\n",
            "\n",
            "Я поздню виду красы.\n",
            "Вблизи русские радость. \n",
            "О — порывайте грехи плоды\n",
            "Вирит венцы любви!\n",
            "\n",
            "Ино страшась нам: виденены \n",
            "Резвый ревнивы грех\n",
            "\n",
            "Резвоскарной кафедрик,\n",
            "Живет Вакхар под кровою,\n",
            "Твои нетощит бесплодушные, \n",
            "Беспечный дух друг ужеленье.\n",
            "\n",
            "Она незапной кругом\n",
            "Герое подарная \n",
            "И убегать тебя\n",
            "В долгах, будь мои, \n",
            "Глупа, в театре кого мечтатель.\n",
            "Печален любви свирели\n",
            "Страстсти, в незложимей стекли,\n",
            "Рассудок ученого квиримы.\n",
            "\n",
            "Поверь, устремившись и вновь, \n",
            "К гермучий лес твой пыли вдали \n",
            "С той бог повес, внемлющих -утешались, \n",
            "Вдруг уединенный хоть в чести царицы \n",
            "Наслаждений беседы властелин. \n",
            "А чем его Стамати белее, \n",
            "А покала в беской приговор \n",
            "И нива обита она \n",
            "Барды когда сойдет веселый \n",
            "Чем он глаза и сидит б поедет.\n",
            "\n",
            "Он лови вечной нежно в тишине \n",
            "В своих сатире избавитый \n",
            "Скажите, читал объемлет строго, \n",
            "Не богат твоих конь. \n",
            "\n",
            "Таков без печальных, ласков бездный, \n",
            "Младое — тоски поднес. \n",
            "\n",
            "Известен мертвеца лучала \n",
            "И тебя красавиц \n",
            "Судьба висшает, она, \n",
            "Помилует окну \n",
            "Из своды поздвенье \n",
            "И видова и нас. \n",
            "Дором страшен ларец ненастной \n",
            "Не тайно любви яземной, \n",
            "Блажен, кто молчит, не что сон \n",
            "В блуях, стран в увядших лет, \n",
            "В которых быть отческив бога \n",
            "Притаясь мне того, не слышавался, \n",
            "Которым хижины — но спесивым власы\n",
            "Пред легкой прорядой и звукнет… \n",
            "И зрит бури пылкие бразден, \n",
            "И мир пламень вечернемую полну, \n",
            "И к творогу растреплять я на дерзал. \n",
            "\n",
            "Ерша твоя покрытывала сидел, \n",
            "А потаенны его глоба \n",
            "Кинжал Леты его Кремита.\n",
            "Оба чем еще за коня верная, \n",
            "Я позовет отуда поэта \n",
            "Потому казакрена выре: \n",
            "\"Беди! краснее, это тот пряхнуть, \n",
            "Повиник, аминь, молясь и богу \n",
            "Клянусь: Ты слезы мои!\" \n",
            "Мирская перед ней беспокойтва \n",
            "И тайно взор твоих Царета. \n",
            "Водою самый шалун, \n",
            "И там в уголку Героевей. \n",
            "Или на речки у Родруга. \n",
            "Год ты пропаду я бояла, \n",
            "Тиселет — и нравагу не подавала \n",
            "Тибки даже милого пятна, \n",
            "И их напьес из ожидала гне, \n",
            "Потинув допит черных воротила, \n",
            "И ему в объятиях облаксла \n",
            "Всем гробе, везде немил ей. \n",
            "Подруга ты собирь, \n",
            "Не такар мне резеленья \n",
            "Скоро спорю, мой старушка \n",
            "Котора, молвил с усмехонья, \n",
            "Которую ты любви \n",
            "Из тебя, мой друг, друг назвал… \n",
            "Свистов! Жизнь твой с Анакамой, \n",
            "Здесь твой наследник, ты восклика \n",
            "Провел меч веселья Лихил, \n",
            "Ты близ Елену Коценской \n",
            "Ты бесплодное мудрецов, \n",
            "Младою светлые взротынья, \n",
            "Как увидел - он ранниты, \n",
            "И снова покину - надел \n",
            "Поцалуй души вкушать. \n",
            "Сижу себе в упоительную \n",
            "Воспоминаня брега, \n",
            "Пора изредка ретиву \n",
            "Перед ней задолил Героев \n",
            "В заботальную пестрел Веселый \n",
            "Он с дивною поседелый \n",
            "В ней в сетклях дальну подругу \n",
            "Храня речки, мой друг, \n",
            "И тебя запрещененья \n",
            "Почивай, как чижик мужей, \n",
            "Бог последний разводил \n",
            "Без тебя тихий трас не лес, \n",
            "И наше в сим приют цевницы. \n",
            "Ничего я придет твоей \n",
            "Придет тлений враг не взлюбит \n",
            "Ручей жаркий, человек, \n",
            "Над королепью вихривый, \n",
            "Радости крючки мой крист \n",
            "Где вижу, вздохнувшенья \n",
            "Счастливый серебря, младой \n",
            "Заверен севера последний \n",
            "Кругом покловников сплевцы! \n",
            "Там не снесла крестница \n",
            "Отпишься, там гнездидесь,\n",
            "И вот уже Анджель Христос вершины, \n",
            "Там, где Вилльгельм помиренны \n",
            "Там ненастно чувств, грязю преньем. \n",
            "С неувшем горячей погряженье: \n",
            "\"Аминьта, ласкал Эрот \n",
            "Прекрасней враждебный сын, \n",
            "Поклониться боярных вершин \n",
            "Ты гимн, явтел безрежный вод. \n",
            "Увы! предаремствуя Сенец \n",
            "Красою девою повет \n",
            "Проникнул ее меть утратил \n",
            "И в сладострастных волосах, \n",
            "И вина пылками одним \n",
            "Великий двух и стыда, \n",
            "Козерней Генералями Россию \n",
            "Идет в тихий дом толпит, \n",
            "Тебя пращ, твой когда строг \n",
            "И волны нежный Аонидвины \n",
            "В чашу вижу под сенью простою, \n",
            "И налиженьем душу обедой. \n",
            "Мой друг, закрый мой, ясный день \n",
            "Пристал красну не страшу \n",
            "Хоть в глубину () кольцо: когда \n",
            "Видят тень иль пробует тот, \n",
            "На только бедно гусит\n",
            "Возбновление моля, \n",
            "Не верю! к тогда же меня. \n",
            "Соседство меня не бывает \n",
            "Барышено печально \n",
            "Напоминают вас \n",
            "Солнце расточенно светницы.\n",
            "\n",
            "Казалось, виден \n",
            "Зевных чудес, чтоб Над бока, \n",
            "Как ветер выжен, беспокоятно. \n",
            "Ты сильной могиле болвною в модный \n",
            "Меня смутные вас ядом. \n",
            "На дом утренние ревнивый \n",
            "Повесит мечты дар.\n",
            "Сокровища труда, Ласси \n",
            "Город бога кровавую \n",
            "Мне Ваше новую мою.\n",
            "\n",
            "Потеряв сей не покровит, \n",
            "И все душе помочтать. \n",
            "О горе, гремит нам союз, \n",
            "Открытий умные всех поток \n",
            "Не предатемся лучайным. \n",
            "Сиют взором, сидит скал \n",
            "И душу твой обед смердеть, \n",
            "Когда я брадуя вдали \n",
            "И в Ислед пощари взоры, \n",
            "Сирийские холодные \n",
            "Поэта вновь не внемлешь \n",
            "Ответай недевственной рукой. \n",
            "С той поры, когда для сердца \n",
            "Сомненьем для незнакома твои \n",
            "Их тени уже мне сладострастья \n",
            "Снимаю ша, прелестная свинца, \n",
            "Какой милогой куплета. \n",
            "\n",
            "Едет бесплодный жизни шутя, \n",
            "Стон один твоя страданья! \n",
            "Сокроюсь надо — радость дорогая! \n",
            "Несчастью пламенный и брат, \n",
            "Одни радости в безмолвии лесть. \n",
            "Перед молвых обПед сиров, \n",
            "Не иместя в друга объятияся, \n",
            "Иль вечной тишины обвиваясь. \n",
            "В день суд веселый чертог сыный \n",
            "Сердца прозраченного героя, \n",
            "Непоставлю тетрадет, \n",
            "И бессмертью для виженный \n",
            "Устех играет в Нерон \n",
            "Как изинь отплет. \n",
            "Бежавно Муз стеклянник \n",
            "Как жань Фоанни Кантерины \n",
            "Чернила караул. Фон-Вися, кому \n",
            "Как чиста возьмет унылыбнется \n",
            "Амур говорит: нет, видит ясно, \n",
            "Втак\n",
            "Слети богов не плюйет, друг.\n",
            "\n",
            "Двухли полет Погибанки, \n",
            "Бежит и меня воскресли, \n",
            "И меня все уже вороты \n",
            "К невежестве, горьких дум.\n",
            "\n",
            "Но где же чласкованы \n",
            "Просторные часы \n",
            "Где же он наклонился \n",
            "Хранит ручьев еще творенье, \n",
            "Так веком погрублен горишь \n",
            "И песенкают хоть во свободы, \n",
            "Заливали поту: \n",
            "Други, хоть по ней моей небесны \n",
            "Тихонько в модной жажденье \n",
            "Где на память умнем. \n",
            "В нуше это слуха \n",
            "Трое дней мстевожит, \n",
            "Проведай вознесй Родрик \n",
            "В его сретить твой игривых \n",
            "К играхм Мытрые венок, \n",
            "Как эрмовного ненастья \n",
            "Задумал не пришлося \n",
            "О чем старушке ног, \n",
            "То, что погонь попал девицы \n",
            "Не сподвоижные крейт небеса, \n",
            "Кто славил Раз\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0rLr2OIrYLmn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}